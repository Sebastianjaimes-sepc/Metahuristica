{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f6a0b8a",
   "metadata": {},
   "source": [
    "# Notebook: Cargar y validar instancia AMPL (.dat)\n",
    "\n",
    "Este notebook implementa un parser ligero para archivos AMPL `.dat` (formato de parámetros \"param ... := ... ;\"), mapea los parámetros a estructuras Python y realiza validaciones básicas para el problema de ruteo / programación de muelles. \n",
    "\n",
    "**Objetivos:**\n",
    "- Parsear el `.dat` proveído (ej: `instancia_generada.dat`).\n",
    "- Construir dataclasses para nodos, camiones y parámetros del problema.\n",
    "- Validar dimensiones y consistencia de parámetros (Dist, tvia, franjas, etc.).\n",
    "- Proveer un resumen legible y una visualización sencilla de la red.\n",
    "\n",
    "> Nota: los parámetros son inmutables — cualquier cambio de parámetros debe hacerse editando el archivo `.dat` original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08fb534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1: Importar librerías y configurar el entorno\n",
    "import re\n",
    "import json\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Tuple, Any\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pprint\n",
    "\n",
    "# Optional visualization packages (install if missing)\n",
    "try:\n",
    "    import networkx as nx\n",
    "    import matplotlib.pyplot as plt\n",
    "except Exception:\n",
    "    nx = None\n",
    "    plt = None\n",
    "\n",
    "# Logging and display options\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"Libraries loaded. Ready to parse instance.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644616bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2: Dataclasses para nodos, camiones y parámetros\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Client:\n",
    "    id: int\n",
    "    escliente: int\n",
    "    esdepo: int\n",
    "    escritico: int\n",
    "    DemE: float = 0.0\n",
    "    DemR: float = 0.0\n",
    "    TS: float = 0.0\n",
    "    MinDC: float = 0.0\n",
    "    MaxDC: float = 24.0\n",
    "\n",
    "@dataclass\n",
    "class Truck:\n",
    "    id: int\n",
    "    Cap: float\n",
    "    CH: float\n",
    "    CF6: float\n",
    "    CF12: float\n",
    "    esHora: int = 1\n",
    "    esF6: int = 0\n",
    "    esF12: int = 0\n",
    "\n",
    "@dataclass\n",
    "class Instance:\n",
    "    clients: Dict[int, Client]\n",
    "    trucks: Dict[int, Truck]\n",
    "    Dist: np.ndarray\n",
    "    tvia: Dict[int, np.ndarray]  # mapping f -> matrix\n",
    "    v: Dict[int, float]\n",
    "    tinic: Dict[int, float]\n",
    "    tfin: Dict[int, float]\n",
    "    params: Dict[str, Any]\n",
    "\n",
    "    def n_nodes(self):\n",
    "        return len(self.clients)\n",
    "\n",
    "print('Dataclasses defined')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0e1cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3: Parser para el archivo AMPL .dat\n",
    "import math\n",
    "\n",
    "def _numtok(s):\n",
    "    \"\"\"Convert a token to int if integer, else float.\"\"\"\n",
    "    if re.match(r\"^-?\\d+$\", s):\n",
    "        return int(s)\n",
    "    try:\n",
    "        return float(s)\n",
    "    except ValueError:\n",
    "        return s\n",
    "\n",
    "\n",
    "def _tokenize_pairs(body: str):\n",
    "    toks = [t for t in re.split(r\"\\s+\", body.strip()) if t != \"\"]\n",
    "    if len(toks) == 0:\n",
    "        return {}\n",
    "    # Scalar single value\n",
    "    if len(toks) == 1:\n",
    "        return _numtok(toks[0])\n",
    "    # Otherwise parse as index value pairs\n",
    "    if len(toks) % 2 == 0:\n",
    "        d = {}\n",
    "        for i in range(0, len(toks), 2):\n",
    "            idx = _numtok(toks[i])\n",
    "            val = _numtok(toks[i+1])\n",
    "            d[idx] = val\n",
    "        return d\n",
    "    # Fallback: return list\n",
    "    return toks\n",
    "\n",
    "\n",
    "def parse_param_blocks(text: str) -> Dict[str, Any]:\n",
    "    \"\"\"Parse all param blocks into a dict name -> parsed content.\"\"\"\n",
    "    res = {}\n",
    "    # Remove C-style comments and lines starting with #\n",
    "    text = re.sub(r\"#.*\", \"\", text)\n",
    "\n",
    "    # Find top-level param blocks: param NAME [optional header] := body ;\n",
    "    pattern = re.compile(r\"param\\s+(\\w+)\\s*(?:([:\\[])[\\s\\S]*?)?:=\\s*([\\s\\S]*?)\\s*;\", re.IGNORECASE)\n",
    "    # Simpler incremental approach: find 'param name' and the following ';'\n",
    "    for m in re.finditer(r\"param\\s+(\\w+)\\b(.*?)\\s*:=\\s*([\\s\\S]*?)\\s*;\", text, re.IGNORECASE):\n",
    "        name = m.group(1)\n",
    "        header = m.group(2).strip()\n",
    "        body = m.group(3).strip()\n",
    "        # Check if matrix form e.g. \" : 0 1 2 := ...\"\n",
    "        if header.startswith(\":\"):\n",
    "            # matrix form\n",
    "            res[name] = parse_matrix_param(header, body)\n",
    "        elif name.lower() == 'tvia' or body.strip().startswith('[*,*,'):\n",
    "            res[name] = parse_tvia(body)\n",
    "        else:\n",
    "            res[name] = _tokenize_pairs(body)\n",
    "    return res\n",
    "\n",
    "\n",
    "def parse_matrix_param(header: str, body: str) -> Dict[Tuple[int,int], float]:\n",
    "    header = header.lstrip(\":\").strip()\n",
    "    cols = [int(x) for x in re.split(r\"\\s+\", header) if x != \"\"]\n",
    "    mat = {}\n",
    "    for line in body.splitlines():\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        parts = [p for p in re.split(r\"\\s+\", line) if p != \"\"]\n",
    "        # First token is row index\n",
    "        row = int(parts[0])\n",
    "        vals = [float(x) for x in parts[1:1+len(cols)]]\n",
    "        for c_idx, col in enumerate(cols):\n",
    "            mat[(row, col)] = vals[c_idx]\n",
    "    # Convert to numpy matrix\n",
    "    n = max([i for i,_ in mat.keys()])\n",
    "    m = max([j for _,j in mat.keys()])\n",
    "    arr = np.zeros((n+1, m+1), dtype=float)\n",
    "    for (i,j),v in mat.items():\n",
    "        arr[i,j]=v\n",
    "    return arr\n",
    "\n",
    "\n",
    "def parse_tvia(body: str) -> Dict[int, np.ndarray]:\n",
    "    # body contains one or more blocks like [*,*,f]: 0 1 2 ... := \\n rows\n",
    "    blocks = {}\n",
    "    # find blocks\n",
    "    block_pattern = re.compile(r\"\\[\\*,\\*,(\\d+)\\]:\\s*(.*?)\\s*:=\\s*([\\s\\S]*?)(?=(\\[\\*,\\*,\\d+\\]:)|$)\", re.IGNORECASE)\n",
    "    for bm in block_pattern.finditer(body):\n",
    "        f = int(bm.group(1))\n",
    "        header = bm.group(2).strip()\n",
    "        block_body = bm.group(3).strip()\n",
    "        cols = [int(x) for x in re.split(r\"\\s+\", header) if x != \"\"]\n",
    "        mat = {}\n",
    "        for line in block_body.splitlines():\n",
    "            line=line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            parts = [p for p in re.split(r\"\\s+\", line) if p != \"\"]\n",
    "            row = int(parts[0])\n",
    "            vals = [float(x) for x in parts[1:1+len(cols)]]\n",
    "            for c_idx, col in enumerate(cols):\n",
    "                mat[(row, col)] = vals[c_idx]\n",
    "        n = max([i for i,_ in mat.keys()])\n",
    "        m = max([j for _,j in mat.keys()])\n",
    "        arr = np.zeros((n+1, m+1), dtype=float)\n",
    "        for (i,j),v in mat.items():\n",
    "            arr[i,j]=v\n",
    "        blocks[f]=arr\n",
    "    return blocks\n",
    "\n",
    "\n",
    "def parse_ampl_dat(path: str) -> Dict[str, Any]:\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    return parse_param_blocks(text)\n",
    "\n",
    "print('Parser functions defined')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad23bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4: Convertir parsed params a estructuras tipadas y validar\n",
    "\n",
    "REQUIRED_PARAMS = ['escliente','esdepo','escritico','esHora','esF6','esF12','Cap','CH','CF6','CF12','DemE','DemR','TS','MinDC','MaxDC','Dist','tvia','v','tinic','tfin','nmuelles','durH','Lc','tcarga']\n",
    "\n",
    "\n",
    "def build_instance(parsed: Dict[str, Any]) -> Instance:\n",
    "    missing = [p for p in REQUIRED_PARAMS if p not in parsed]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Faltan parámetros requeridos en .dat: {missing}\")\n",
    "\n",
    "    # clients: assemble for indices in parsed['escliente'] dict\n",
    "    escliente = parsed['escliente'] if isinstance(parsed['escliente'], dict) else {}\n",
    "    esdepo = parsed['esdepo'] if isinstance(parsed['esdepo'], dict) else {}\n",
    "    escritico = parsed['escritico'] if isinstance(parsed['escritico'], dict) else {}\n",
    "\n",
    "    all_node_ids = sorted(set(list(escliente.keys()) + list(esdepo.keys())))\n",
    "    clients = {}\n",
    "    for nid in all_node_ids:\n",
    "        clients[nid] = Client(\n",
    "            id=nid,\n",
    "            escliente=int(escliente.get(nid,0)),\n",
    "            esdepo=int(esdepo.get(nid,0)),\n",
    "            escritico=int(escritico.get(nid,0)),\n",
    "            DemE=float(parsed['DemE'].get(nid,0.0)),\n",
    "            DemR=float(parsed['DemR'].get(nid,0.0)),\n",
    "            TS=float(parsed['TS'].get(nid,0.0)),\n",
    "            MinDC=float(parsed['MinDC'].get(nid,0.0)),\n",
    "            MaxDC=float(parsed['MaxDC'].get(nid,24.0)),\n",
    "        )\n",
    "\n",
    "    # trucks\n",
    "    Cap = parsed['Cap']\n",
    "    CH = parsed['CH']\n",
    "    CF6 = parsed['CF6']\n",
    "    CF12 = parsed['CF12']\n",
    "    esHora = parsed['esHora'] if isinstance(parsed['esHora'], dict) else {}\n",
    "    esF6 = parsed['esF6'] if isinstance(parsed['esF6'], dict) else {}\n",
    "    esF12 = parsed['esF12'] if isinstance(parsed['esF12'], dict) else {}\n",
    "\n",
    "    truck_ids = sorted(set(list(Cap.keys())))\n",
    "    trucks = {}\n",
    "    for tid in truck_ids:\n",
    "        trucks[tid] = Truck(\n",
    "            id=tid,\n",
    "            Cap=float(Cap.get(tid)),\n",
    "            CH=float(CH.get(tid)),\n",
    "            CF6=float(CF6.get(tid)),\n",
    "            CF12=float(CF12.get(tid)),\n",
    "            esHora=int(esHora.get(tid,1)),\n",
    "            esF6=int(esF6.get(tid,0)),\n",
    "            esF12=int(esF12.get(tid,0)),\n",
    "        )\n",
    "\n",
    "    Dist = parsed['Dist'] if isinstance(parsed['Dist'], np.ndarray) else np.array([])\n",
    "    tvia = parsed['tvia'] if isinstance(parsed['tvia'], dict) else {}\n",
    "    v = {int(k): float(v) for k,v in parsed['v'].items()} if isinstance(parsed['v'], dict) else {}\n",
    "    tinic = {int(k): float(v) for k,v in parsed['tinic'].items()} if isinstance(parsed['tinic'], dict) else {}\n",
    "    tfin = {int(k): float(v) for k,v in parsed['tfin'].items()} if isinstance(parsed['tfin'], dict) else {}\n",
    "\n",
    "    params = {k:v for k,v in parsed.items() if k not in ['escliente','esdepo','escritico','Cap','CH','CF6','CF12','DemE','DemR','TS','MinDC','MaxDC','Dist','tvia','v','tinic','tfin']}\n",
    "\n",
    "    inst = Instance(clients=clients, trucks=trucks, Dist=Dist, tvia=tvia, v=v, tinic=tinic, tfin=tfin, params=params)\n",
    "\n",
    "    # Basic consistency checks\n",
    "    n_nodes = len(clients)\n",
    "    if Dist.shape[0] != n_nodes or Dist.shape[1] != n_nodes:\n",
    "        logger.warning(f\"Dist matrix shape {Dist.shape} does not match number of nodes {n_nodes}\")\n",
    "\n",
    "    if len(tvia) == 0:\n",
    "        logger.warning(\"tvia not parsed as blocks per franja; check file format\")\n",
    "\n",
    "    return inst\n",
    "\n",
    "print('Builder/validator ready')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af20b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 5: Uso de ejemplo con la ruta que indicaste\n",
    "\n",
    "dat_path = r\"C:\\Users\\sjaim\\Downloads\\instancia_generada.dat\"\n",
    "\n",
    "try:\n",
    "    parsed = parse_ampl_dat(dat_path)\n",
    "    print('Parsed parameters:', ', '.join(parsed.keys()))\n",
    "    inst = build_instance(parsed)\n",
    "    print('\\nInstance summary:')\n",
    "    print('Nodes:', inst.n_nodes())\n",
    "    print('Trucks:', len(inst.trucks))\n",
    "    print('Franjas (v):', sorted(inst.v.items()))\n",
    "    print('nmuelles:', parsed.get('nmuelles'))\n",
    "    # Show a sample tvia for franja 1 from node 0->1\n",
    "    if isinstance(inst.tvia, dict) and 1 in inst.tvia:\n",
    "        print('Sample tvia[0,1,1] =', inst.tvia[1][0,1])\n",
    "    else:\n",
    "        print('tvia[1] not available')\n",
    "except FileNotFoundError as e:\n",
    "    print('File not found locally. When you run this notebook, place the .dat at the path above or update dat_path.')\n",
    "except Exception as e:\n",
    "    print('Error parsing/building instance:', e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e0756a",
   "metadata": {},
   "source": [
    "## Section 6: Visualización y resumen rápido\n",
    "\n",
    "- Dibuja un grafo simple con NetworkX si está disponible.\n",
    "- Resumen de demandas, ventanas y criticidad por cliente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f75478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization code (if networkx available)\n",
    "if nx is None:\n",
    "    print('NetworkX not available; install networkx and matplotlib to enable plotting')\n",
    "else:\n",
    "    G = nx.DiGraph()\n",
    "    for nid, c in inst.clients.items():\n",
    "        G.add_node(nid, escliente=c.escliente, escritico=c.escritico, esdepo=c.esdepo)\n",
    "    # Add a few sample edges for visualization (not full graph)\n",
    "    nodes = list(inst.clients.keys())\n",
    "    for i in range(len(nodes)-1):\n",
    "        G.add_edge(nodes[i], nodes[i+1])\n",
    "\n",
    "    colors = []\n",
    "    for n in G.nodes:\n",
    "        if inst.clients[n].esdepo == 1:\n",
    "            colors.append('red')\n",
    "        elif inst.clients[n].escritico == 1:\n",
    "            colors.append('orange')\n",
    "        else:\n",
    "            colors.append('lightblue')\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    pos = nx.spring_layout(G, seed=1)\n",
    "    nx.draw(G, pos, with_labels=True, node_color=colors, node_size=600)\n",
    "    plt.title('Nodos: rojo=depot, naranja=critico, azul=cliente')\n",
    "    plt.show()\n",
    "\n",
    "# Quick tables\n",
    "clients_df = pd.DataFrame([{ 'id':c.id, 'escliente':c.escliente, 'escritico':c.escritico, 'DemE':c.DemE, 'DemR':c.DemR, 'TS':c.TS, 'MinDC':c.MinDC, 'MaxDC':c.MaxDC } for c in inst.clients.values()])\n",
    "print('\\nClientes summary:')\n",
    "print(clients_df.head(15))\n",
    "\n",
    "print('\\nDemandas resumo: min, mean, max:', clients_df['DemE'].min(), clients_df['DemE'].mean(), clients_df['DemE'].max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf1e433",
   "metadata": {},
   "source": [
    "## Next steps / TODOs\n",
    "- Implement the single-vector encoding and genetic operators (RBX, cut-and-fill mutation, SWAP, INSERT).\n",
    "- Implement route simulator and feasibility checker (capacity over time, windows, lunch, muelles availability).\n",
    "- Implement muelle scheduler using weights w1..w5 and integrate with simulator.\n",
    "- Add unit tests and small instance experiments; later, add MILP baseline with PuLP for comparison.\n",
    "\n",
    "> Si quieres, continúo ahora implementando la codificación GA y los operadores en el siguiente notebook (recomiendo un notebook separado `02_ga_operators.ipynb`) y construir pruebas unitarias sobre tu instancia.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
